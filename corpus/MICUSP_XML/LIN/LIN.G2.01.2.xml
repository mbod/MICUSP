<TEI.2><teiHeader><fileDesc><titleStmt><title>A Dynamic POS-Tagging-Learned Approach of Chinese WSD</title><author>anonymized</author></titleStmt><wordcount>4083</wordcount><editionStmt><edition>Version 1.0</edition></editionStmt><extent>insert_extent</extent><publicationStmt><idno>LIN.G2.01.2</idno><publisher>The University of Michigan</publisher><pubPlace>Ann Arbor, Michigan, USA</pubPlace><date>2007</date><availability><p>The MICUSP project is owned by the Regents of the University of Michigan, who hold the copyright. The database has been developed by the English Language Institute. The original text submissions are stored electronically in the English Language Institute and may be consulted by bona fide researchers under special arrangements. The database is available by password on the World Wide Web for study, teaching and research purposes. Copies of the text files may be distributed, as long as either this statement of availability or the citation given below appears in the text. However, if any portion of this material is to be used for commercial purposes, such as for textbooks or tests, permission must be obtained in advance and a license fee may be required. For further information about copyright permissions, please inquire at micusp@umich.edu.</p><p>The recommended citation for MICASE is: Ädel, A., J. M. Swales, and J. Sielaff. (2007) The Michigan Corpus of Upper-level Student Papers. Ann Arbor, MI: The Regents of the University of Michigan.</p></availability></publicationStmt><seriesStmt><title>The Michigan Corpus of Upper-level Student Papers (MICUSP)</title></seriesStmt><sourceDesc><p>This document derives from an unpublished, student-authored paper contributed in electronic format.</p></sourceDesc></fileDesc><encodingDesc><projectDesc><p>For information on corpus compilation and encoding practices, see the MICUSP Handbook.</p></projectDesc><classDecl><taxonomy><category><catDesc>departments</catDesc><category id="BIO"><catDesc>Biology</catDesc></category><category id="CEE"><catDesc>Civil and Environmental Engineering</catDesc></category><category id="CLS"><catDesc>Classical Studies</catDesc></category><category id="ECO"><catDesc>Economics</catDesc></category><category id="EDU"><catDesc>Education</catDesc></category><category id="ENG"><catDesc>English</catDesc></category><category id="IOE"><catDesc>Industrial and Operational Engineering</catDesc></category><category id="LIN"><catDesc>Linguistics</catDesc></category><category id="MEC"><catDesc>Mechanical Engineering</catDesc></category><category id="NRE"><catDesc>Natural Resources</catDesc></category><category id="NUR"><catDesc>Nursing</catDesc></category><category id="PHI"><catDesc>Philosophy</catDesc></category><category id="PHY"><catDesc>Physics</catDesc></category><category id="POL"><catDesc>Political Science</catDesc></category><category id="PSY"><catDesc>Psychology</catDesc></category><category id="SOC"><catDesc>Sociology</catDesc></category></category><category><catDesc>student levels</catDesc><category id="G0"><catDesc>senior undergraduate</catDesc></category><category id="G1"><catDesc>first-year graduate</catDesc></category><category id="G2"><catDesc>second-year graduate</catDesc></category><category id="G3"><catDesc>third-year graduate and up</catDesc></category></category><category><catDesc>student-reported text types</catDesc><category id="CaseStudy"><catDesc>case study</catDesc></category><category id="Critique"><catDesc>critique</catDesc></category><category id="LiteratureReview"><catDesc>literature review</catDesc></category><category id="ResearchProposal"><catDesc>research proposal</catDesc></category><category id="ResponsePaper"><catDesc>response paper</catDesc></category><category id="TechnicalOrLabReport"><catDesc>technical report or lab report</catDesc></category><category id="TermPaper"><catDesc>term paper</catDesc></category><category id="OtherTextType"><catDesc>other</catDesc></category></category><category><catDesc>approximate preparation times</catDesc><category id="OneToThreeDays"><catDesc>1-3 days</catDesc></category><category id="FourToTenDays"><catDesc>4-10 days</catDesc></category><category id="TwoToThreeWeeks"><catDesc>2-3 weeks</catDesc></category><category id="ThreeToSixWeeks"><catDesc>3-6 weeks</catDesc></category><category id="OneFullSemester"><catDesc>one full semester</catDesc></category></category><category><catDesc>sources of feedback</catDesc><category id="NoFeedback"><catDesc>no feedback from others</catDesc></category><category id="PrimaryInstructor"><catDesc>primary course instructor</catDesc></category><category id="GSI"><catDesc>Graduate Student Instructor (teaching assistant)</catDesc></category><category id="WritingTutor"><catDesc>writing tutor</catDesc></category><category id="FriendOrClassmate"><catDesc>friend or classmate</catDesc></category><category id="OtherFeedback"><catDesc>other</catDesc></category></category></taxonomy></classDecl></encodingDesc><profileDesc><creation>Paper submitted to instructor in Dec 2001</creation><particDesc><person id="P101" sex="f" age="TwentyFourToThirty"><affiliation>Linguistics</affiliation><firstLang>Mandarin</firstLang><dominantLang>Mandarin</dominantLang><englishInPrimarySchool value="NO"/><englishInSecondarySchool value="NO"/><englishInUndergraduate value="NO"/></person></particDesc><textClass><catRef target="LIN G2 Proposal TermPaper ThreeToSixWeeks NoFeedback"><classification><primary>Proposal</primary><secondary/></classification><classification><primary>Proposal</primary><secondary/></classification><features><feature type="Definitions"/><feature type="Methodology section"/><feature type="Tables, graphs or figures"/><feature type="Reference to sources"/></features></catRef></textClass></profileDesc><revisionDesc><change><date>2008-04-16</date><respStmt><name>Geoffrey Ho</name><resp>coder</resp></respStmt><item>manual adjustment of tags</item></change><change><date>2007-05-22</date><respStmt><name>Gregory Garretson</name><resp>programmer</resp></respStmt><item>initial (automated) encoding in XML</item></change></revisionDesc></teiHeader><text><body><div type="opener"><head>A Dynamic POS-Tagging-Learned Approach of Chinese WSD</head><p/></div><div type="main"><div type="head_1"><head>1.Introduction</head><div type="head_2"><head>1.1 What is word sense disambiguation (WSD)</head><p>Word sense tagging is essentially a word sense disambiguation (WSD) process.  Its main task is to assign a proper word sense code for every word in the input sentences according to word sense tagging set and the context. This sense coding can take four forms: 1) The corresponding code of a word in a semantic domain dictionary. 2)  The sequence number of an explanation text for a word in an ordinary dictionary. 3) The corresponding target word of a word in a translation dictionary. 4) The concept definition item in a concept dictionary (e.g. the concept definition in How-Net). Word sense tagging includes two processes which are tagging assignment and tagging disambiguation. Tagging assignment is to assign a set of possible or potential word sense tagging to every word. Tagging disambiguation is to choose a proper word sense that satisfies the context.</p><p>Word sense tagging is considered to be the biggest unsolved issue at word level in natural language processing.  Although word sense tagging is similar to POS  tagging, it is more abstract and difficult because of its focus on the aggregation relation of word meanings.  As early as 1950s, people started to work on the research of WSD.  A large variety of methodologies have been put forwarded.  However, at present stage, most of the current systems can handle the WSD for only a very limited number of multi-sense words.  There is still long way to go for WSD to reach the present level of POS tagging. In Chinese NLP research area, WSD has been a rarely touched topic and relatively little effort has been paid to this important issue. This paper is going to propose a technically feasible approach to improve the WSD of Chinese based on the relative maturity of POS tagging system.     </p></div><div type="head_2"><head>1.2 What is the research problem</head><p>In order to make the computer have the same word sense discrimination ability as human, we must store in computer grammars and semantic and pragmatic rules as well.  WSD methodologies can be grouped into four categories in terms of the methodologies used to acquire knowledge.</p><p>1)WSD based on dictionary knowledge.  Machine readable dictionaries and semantic domain dictionaries provide sufficient knowledge on word usages and word senses and therefore can be used as a resource for WSD. Machine readable dictionary has become the major knowledge resource for WSD since 80s. Typical machine readable dictionary-based methodology for WSD works this way:  It first computes the coverage of each word sense for ambiguous words and the senses of words in the context and then it chooses the sense with maximum overlapping as the current word sense.</p><p>Unfortunately, the correctness rate for this methodology is only 50% to 70%.  There are several reasons for this comparatively low accuracy: Firstly, traditional machine readable dictionary-based methodology does not make full use of the useful information of the phrases and examples in the dictionary. Secondly, the word sense definition sentences in machine readable dictionaries are normally very short, and it is hence difficult to calculate the overlapping of those two kinds of definitions.  In many cases, there are zero overlapping between the word sense definitions of the ambiguous word and context words. Thirdly, in practice, the inevitable combination explosion also greatly restricts the application of this method.  Finally. the dictionary is for human use and not for machine use. In addition, there is also inconsistence in the dictionary itself , all these raise difficulty for knowledge retrieval.</p><p>Knowledge in semantic domain dictionary is also widely used for WSD. Unlike the methodology used by machine readable dictionary, semantic domain dictionary organizes the words into hierarchy levels in terms of word senses, providing connections between words.  WordNet (Miller 1990) and <q type="title">Roget's Thesaurus</q> are the most famous English semantic domain dictionaries; <q type="title">Chinese Thesaurus</q>(J.Mei,1996) and <q type="title">Knowledge Net</q> (Z.Dong, 1999) are most commonly used Chinese semantic resources.</p><p>2) WSD based on rules: This methodology depends on the language knowledge of language experts. It constructs rule sets, analyzing the ambiguous words and their contexts, and chooses the word senses that satisfy the restriction rules.  Generally speaking, the rules describe the elements that are licensed to modify the ambiguous words and those disallowed to modify the ambiguous words. </p><p>3) WSD based on language corpus: The advent of this methodology marks a new age in NLP and gradually becomes indispensable for WSD research.  It can be categorized into supervised and unsupervised WSD.  The former collects the WSD knowledge from the processed and tagged material; The latter collects WSD knowledge from the raw and tagged material . The essence of the corpus-based WSD is to determine different meanings of words in a context by automatic or semi-automatic learning of the corpus.  It can be either statistics-based or example-based. The former collects statistical context proof that support the specific sense of an ambiguous word in a certain context, and the proof is used for WSD of the new input sentences.  This method usually collects statistical data of collocation between words and word senses. Example-based method uses statistics almost without turning to any language structural knowledge.  What it needs is large quantity of regular parallel sentences. It's easy to implement but hard to obtain a large number of parallel sentences.</p><p>4) WSD based on combination of different methods: After several years' efforts, more and more researchers now tend to combine multiple methods to realize WSD.  The method combines a variety of methods to get better WSD performance.  The combination of knowledge resources will extend the potentially useful knowledge for WSD, such as dictionary information, collocation information, domain knowledge, syntactic restriction and all of the other potential heuristic information .  Based on the word sense system in <q type="title">Chinese Thesaurus</q>, C.Huang and J.Li adopted the unsupervised learning method to process large scale corpus and constructed the <q type="term">categorizing machine</q> for WSD use.  </p></div><div type="head_2"><head>1.3 Who might be interested in a proposal to improve WSD performance</head><p>The WSD is itself an intermediate process and hence an indispensable middle layer for most NLP tasks. Any improvement of WSD would interest people employed in the following fields:</p><div type="head_3"><head>1.3.1 Machine translation</head><p>The WSD in machine translation has its own characteristics in that it uses target word to distinguish different word senses.  How to find the target word in machine translation is an important issue and how the WSD is solved directly affects the quality of the translated text.  For example, in the English-Chinese translation system, the word <q>interest</q> has two target words in Chinese <q>利益</q> and <q>利息</q>.  When translating this word, the system should use the context to decide which  to choose in a specific context.</p></div><div type="head_3"><head>1.3.2 Information search and information processing </head><p>In information search and processing, due to the multiple meanings of a word, the system sometimes would come up with the texts that containing the same word with different meaning.  For example, someone may use <q>材料</q> as key word if he is searching for some reference related to a file.  If the search is solely based on these characters, then it will give you all the papers containing this word most of which are related to <q>manufacturing material</q> and do not actually mean <q>files</q>. Therefore, the word sense is important in information search.</p></div><div type="head_3"><head>1.3.3 Sentence analysis</head><p>The word sense also plays a big role when analyzing the grammatical structure of sentences. Grammatical ambiguity is common in many languages and solution to the problem is to introduce word sense. For example, <q>参观图书馆的大厅(visiting library's hall)</q> and <q>参观图书馆的人们(the people who are visiting library)</q> share the POS sequence of <q>verb+noun+de+noun</q>, but they have very different sentence structures.  </p></div><div type="head_3"><head>1.3.4 Natural language understanding</head><p>When defining the semantic structure of a sentence, we have to consider the word sense of every word in the sentence.  The semantic structure of a sentence (e.g Case structure) can be obtained only when the word sense of every word in the sentence is known.</p></div><div type="head_3"><head>1.3.5 Speech recognition and pronunciation-character transformation</head><p> The word-based N-yuan model takes into consideration only the continuous relations between words, thus there exists sentences with no meaning connections between words in the recognition result. With the introduction of word sense, the continuous relations on the word sense level are provided; therefore it avoids this kind of error mentioned above in some sense.</p><p>In summary, as an important process in NLP, the WSD research has great significance both theoretically and practically.  The result of WSD research can be directly applied in many aspects of NLP.</p></div></div></div><div type="head_1"><head>2. An Overview of Chinese word sense tagging system and ambiguity distribution of Chinese words</head><div type="head_2"><head>2.1 <q>Chinese Thesaurus</q> as a popular word sense dictionary</head><p>Like POS tagging, word sense tagging also need a tagging system which can provide a description of the word sense categorization principles. Since the dictionary is the basis of word sense definition, a perfect dictionary should be the precondition of WSD. <q type="title">Chinese Thesaurus</q> is the only machine-readable semantic dictionary in  current Chinese information processing.  The compiler of <q type="title">Chinese Thesaurus</q> defines the word categorizing rules based on characteristics of usages of Chinese words. Technically, it adopts the coding system of <q type="title">The Longman Lexicon of Contemporary English</q>. <q type="title">Thesaurus</q> divides word senses into large, medium, and small categories.  There are 12 large categories, 94 medium categories and 1428 small categories.  Uppercase letters represent large categories; Lowercase letters and numbers represent the medium and small categories respectively.  The twelve large categories include: people(A), object(B), time and space(C), abstract object(D), characteristics(E), action(F), psychological activity(G), activity(H), phenomenon and state(I), connection(J), auxillaries(K) and honorific(L). It describe a word sense categorizing system from up to down, from broad concept to concrete word senses. The organization of the word sense coding system is as follows:</p><p>          &lt;word sense coding&gt;::=&lt;large category&gt;&lt;medium category&gt;&lt;small category&gt;</p><p>	          &lt;large category&gt;::=&lt;uppercase English letter&gt;</p><p>	          &lt;medium category&gt;::=&lt;lowercase English letter&gt;</p><p>                &lt;small category&gt;::=&lt;digit&gt;&lt;digit&gt;</p><p>In the small category, the words are further grouped into smaller word clusters synonyms.  Every word cluster has a title word.  There are totally 3925 title words altogether and word clusters are further represented by a two digit number.  For example, the coding of the word <q>觉悟</q> is <q>Ga15</q>. It is represented in <q type="title">Chinese Thesaurus</q> as:</p><p>     Ga15  醒悟   懂事</p><p>	            醒悟   觉悟   省悟   觉醒   清醒   醒   如梦初醒      大梦初醒 ……</p><p>	            懂事  记事儿    开窍    通窍	</p><p>    </p><p>There are two word clusters in <q>Ga15</q>. One is composed of the words representing <q>conscious</q> and the other is composed of words representing <q>know something</q>.  Therefore, the further word sense coding for <q>觉悟</q> is <q>Ga1501</q> which can be called sub small category coding. Multi-sense words are grouped into different word clusters in terms of their word senses.  For example, 材料 has three word senses in <q type="title">Thesaurus</q>: (1) stuff that can be made into product. (2) stuff provided for the content of a work or for references. (3) a person that has talent to do something.  The corresponding coding for each word senses are <q>Ba06</q>,<q>Dk17</q> and <q>A103</q>.  In this article, the word sense coding in <q type="title">Thesaurus</q> is directly used to represent word sense. Multi-sense word has multiple word sense codings and the length of the coding is 4.</p></div><div type="head_2"><head>       2.2 Chinese Word Ambiguity Degree and Distribution</head><p><q type="title">Chinese Thesaurus</q>s has a vocabulary of more than 50,000 words. The following table shows the distribution of multi-sense words in Chinese:</p><p>Table1.   Multi-sense word distribution in <q type="title">Chinese Thesaurus</q></p><gap desc="table"/><p>     The statistics above shows that  <q type="title">Thesaurus</q> has 42724 mono-sense words and 7370 multi-sense words.  Besides that, it also shows that 14.8% of the words are multi-sense words. We note also that shorter words have higher chance to be ambiguous and also tend to be more ambiguous. </p><p>According to the word sense and ambiguity type of the word sense Chinese word can be categorized into mono-sense word, category ambiguous word, non-category ambiguous word and blending ambiguous word.  Among these types, the WSD of the  non-category ambiguous word is most difficult because it needs more contexts.  The research on the distribution of these different types helps to adopt different tagging strategies to different words.  In order to have a better understanding of how the words distribute with respect to word senses, we collect the statistical data on static distribution of word sense based on Chinese Thesaurus. The results are in table2 as follows:</p><p>Table2  word distribution with respect to word sense in Chinese Thesaurus</p><gap desc="table"/><p>Word sense distribution is in close relation with the technical principles of a dictionary and specific word sense category system. The data in Table 2 and Table 3 may not perfectly reflect the relations among POS, word sense and  word distribution, we can still see clearly the following general trends: Firstly, in Chinese, most words are mono-sense words, with a dominating percentage of 81.46-87.69%. Secondly, multi-sense words, although occupying only around a quarter of the whole vocabulary, are used much more frequently than mono-sense words. More commonly used words usually have a higher degree of ambiguity.1 Thirdly, multi-sense words are not evenly distributed with respect to their POS.  Thirdly, the granularity of word sense categorizing has great effects on WSD. The smaller the granularity, the higher percentage of the multi-sense words, and also the bigger average word senses. On the other hand, the smaller the granularity, the higher percentage of non-category multi-sense words and the lower percentage of category multi-sense words. There is little effect on the percentage of blending multi-sense words though. In a word, word sense ambiguity becomes more serious if we adopt a comparatively smaller granularity when define the word sense tagging system. Finally, one conspicuous characteristic of Chinese word is that its POS is morphologically unmarked. And there is always the extreme claim that Chinese words do not even have POS, it only gets POS when it comes into a specific sentence(<q> 依句辩品，离句无品</q>) . This makes WSD more important and complicated for Chinese words than for English words. And this same fact leads us to believe that the approach we are proposing in this article should work much better for Chinese than for English. </p></div></div><div type="head_1"><head>3. Our proposal</head><p>   </p><p>Compared with WSD in English, the research of WSD in Chinese has been drawn much less attention. The method we put forward here is based on the characteristic of Chinese words, we see that POS information is relatively not closely related to lexicon items in dictionary, but once recognized in a given context, brings much information which will be useful for WSD. Our basic idea is that given the high accuracy of present POS tagging system, we are going to count on the output of POS tagging of a text we are processing. And we are going to let the POS information take over the WSD task in the first place. Given the characteristics of Chinese words we discussed above, we believe it a feasible and effective way of improving the WSD of Chinese text processing. We propose the following steps of this approach. </p><div type="head_2"><head>3.1 Text segmentation and POS tagging</head><p>In current Chinese tagging systems, these two steps can be realized at the same time. Among currently existent systems, the one developed by the Institute of Computational Linguistics of Peking University has reached the accuracy of 96.8%. And in our final evaluation part of this paper, we are also using this system.</p></div><div type="head_2"><head>3.2 Word sense label assignment base on <q>Chinese Thesaurus</q></head><p>The label assignment of word sense is to assign a set of possible or potential word senses to each word. Since we have tagged the POS for the input sentences before word sense tagging.  Therefore, the word sense label assigning process can be described as follows: Firstly, to input the word sequence with POS tagging  as <gap desc="formula"/> ( w is Chinese word, p is its POS in this specific context). Secondly, to assign a set of potential word sense codings for each word in the input according to <q type="title">Chinese Thesaurus</q>. Hence we get a tagging cluster: <gap desc="formula"/> as output.</p><p>In this paper, the process of automatic label assignment is actually the process of link search and dictionary search.  To facilitate the search, we put all of the words searched in the dictionary together with their word sense codings in a link. The structure of the link is as follows:</p><p>Struct word{ // structure used to store the word sense codings</p><p>      Char cla[10];// char array to store the word sense codings</p><p>      Struct word *chain; // pointing to the next node</p><p>};</p><p>struct node{ //structure used to store words </p><p>   char nam[20]; //to store Chinese words</p><p>   struct node * link; // pointing to the next node</p><p>   struct word * next;//pointing to the word sense node of the word</p><p>};</p><p>We can further illustrate this process as follows:</p><gap desc="figure"/><p>The technical process of this word sense label assignment is made up of three steps:</p><p>1) Read in a Chinese word </p><p>2) Search in the link, if information about that word is found there, then use the information do the tagging and get the output. When this is over, go back to the first step. If not found, then go to step 3.</p><p>3) Search the word sense information in the machine readable dictionary <quote>Thesaurus</quote>, if found, then assign all of its word senses at the output. If not found, set the word sense tagging to null and then go to step 1.</p></div><div type="head_2"><head>3.2 WSD based on the dynamic corresponding information between POS and word senses </head><p>We mentioned earlier that the twelve large upper-case letters represent twelve semantic fields that word senses may fall into. <q>Thesaurus</q> does not group words under the name of POS, but  by investigating into <q>Thesaurus</q> and comparing it with <q>Modern Chinese Dictionary</q>, we  can see the correspondence between word sense and POS as shown in Table 3:</p><p>Table 3 The correspondence between word sense and POS</p><gap desc="table"/><p>We can see that there are strong relation between word sense tagging and POS tagging . For example, A, B and D in <q type="title">thesaurus</q> are mostly noun. This correspondence relation is very useful for WSD improvement.</p><p>Given the correspondence relation between the POS and word sense, the last step of your method is:  Cross out all of the word senses that are not included in the correspondence list of the specific tagged POS <i>pi</i> of that word. And for some words, if the results are null, then just keep the original  word sense tagging in order to guarantee the reliability of the final accuracy of the WSD. By this final step, we are descreasing the average sensed of the words in the text to a noticeable extent and hence improve the WSD performance.</p></div></div><div type="head_1"><head>4. Evaluation of this approach</head><p>We are expecting that the application of our approach at the first step of WSD will decrease the average sense numbers of the words in a given text. For the sake of time, we tested our approach manually on one news article from bbs.mit.edu. If the ambiguities are well decreased by our approach for this article, we have good reasons to believe that this approach will improve Chinese WSD to a considerable extent. And the advantage of this approach is that it is easy to apply and can be freely combined with any other WSD algorithms. We segmented and tagged the news article using segmentation and tagging system developed by the Institute of Computational   Linguistics of Peking University, flowing is the output of the tagging:</p><p>中国/ns 领事馆/n 放宽/v 来自/v 中国/ns 大陆/n 新/d 移民/v 的/u 护照/n 申请/v 资格/n 的/u 谣言/n ，/w 连日来/l 在/p 纽约/ns 华人/n 社区/n 传/v 得/u 沸沸扬扬/I ，/w 以讹传讹/I ，/w 连续/a 几/m 天/q 都/d 有/v 上千/m 人/n 新/d 移民/v 涌/v 向/p 位于/v 曼哈顿/ns 的/u 中国/ns 驻/v 纽约/ns 总/v 领/v 馆/Ng ，/w 在/p 中/f 领/v 馆/Ng 的/u 签证/n 组/n 门前/s 大/a 排长/n 龙/n 。/w 中/f 领/v 馆/Ng 官员/n 对外/v 声明/v ，/w 目前/t 中/f 领/v 馆/Ng 对/p 纽约/ns 地区/n 新/d 移民/v 申请/v 中国/ns 护照/n 的/u 需要/v 进行/v 最/d 新/a 的/u 意见/n 调查/v ，/w 听取/v 纽约/ns 地区/n 侨/Ng 团/v 及/c 侨胞/n 的/u 有关/p 护照/n 侨/Ng 团/v 的/u 反映/v 。/w 希望/v 作为/p 中国/ns 护照/n 申请/v 有关/vn 部门/n 的/u 研究/v 参考/v ，/w 但/d 从未/d 改变/v 护照/n 申请/v 发放/v 政策/n ，/w 希望/v 侨胞/n 们/k 不/d 要/v 轻信/v 谣言/n 。/w </p><p>The following is the word sense list of the 118 words in the article according to <q type="title">Chinese Thesaurus</q>, those with <q>* </q>after them indicate that it is crossed out after the WSD:</p><p>谣言   Da19</p><p>轻信   Gb14</p><p>不       Ed28*           Ed23*           Ee19*           Ee38*           Ie01           Ka18</p><p>要      Ag04*           Dk10*           Ed28*           Gb04            Gc03          Hi25</p><p>           Jc05              Kc01*           Kc08*</p><p>侨胞  Ad01</p><p>们____</p><p>希望  Df08*           Gb04</p><p>政策  Di09</p><p>发放   Hc07</p><p>申请  Hc15</p><p>护照_____</p><p>改变  Ih02</p><p>从未______</p><p>但       Ka07        Kc03*</p><p>参考    Hg12</p><p>研究    Gb01       Hg14</p><p>的       Bo29*      Ed01*      Kd01</p><p>部门   Di09         Dm01</p><p>有关   Je01*        Kb04</p><p>申请   Hc15</p><p>护照   ____</p><p>中国   Hc15</p><p>作为   Di02         Ja01(both of these two codings are not included in the list of POS, and </p><p>            we keep these two)</p><p>希望    Df08*        Gb04</p><p>反映    Hc15          Ja03</p><p>的        Bo29*       Ed01*       Kd01</p><p>团       Bb04*        Br09*       Di09*      Di10*      Dn08*      Ea13*      Fa34</p><p>侨      Ad01</p><p>护照   ____</p><p>有关  Je01*         Kb04</p><p>的      Bo29*       Ed01*       Kd01</p><p>侨胞  Ad01</p><p>及      Je12*     Kb02*     Kc01</p><p>侨     Ad01</p><p>团     Bb04*      Br09*      Di09*      Di10*      Dn08*      Ea13*      Fa34</p><p>地区 Cb08 (this coding is not included in the list of POS, and we keep it)</p><p>纽约____</p><p>听取  Fc05</p><p>调查  Hc18</p><p>意见  Df14</p><p>的      Bo29*      Ed01*      Kd01</p><p>新      Eb22*      Eb28*      Ka12</p><p>最      Ka02</p><p>进行   Ig03</p><p>需要  Df07*        Jc05</p><p>的     Bo29*       Ed01*      Kd01</p><p>护照  ____</p><p>中国   Hc15 </p><p>申请   Hc15</p><p>移民____</p><p>新      Eb22*     Eb28*     Ka12</p><p>地区  Cb08</p><p>纽约  ____</p><p>对      Dk17*     Dn08*    Eb02*    Ed12*    Fa26*   Hc18*   Hi08*   Hi18*    Jc02*</p><p>          Kb01       Kb04      Kb07</p><p>领      Bk05*      Bq04*    Dn08*   Hf04      Hi27      Je14</p><p>馆      Dm05</p><p>中      Aj14*      Ca06*      Ca08*     Cb04     Cb05     Da05*    Ea03*    Ed06*</p><p>          Ed47*       Ie11*       Je13*     </p><p>目前  Ca10</p><p>声明  Dk14*      Hi13</p><p>对外  _____ </p><p>官员   ____</p><p>馆      Dm05</p><p>领      Bk05*     Bq04*      Dn08*      Hf04       Hi27        Je14</p><p>中      Aj14*      Ca06*      Ca08*     Cb04     Cb05     Da05*    Ea03*    Ed06*</p><p>          Ed47*       Ie11*       Je13*     </p><p>龙      Dd15        Dh03        Ed57*</p><p>排长   Ae10</p><p>大      Ah04*      Ah05*      Bf02*     Dj05*     Dn04*      Ea03       Eb04</p><p>           Ec05        Ed26        Ed38       Ka01* </p><p>门前   ____</p><p>组       Di09        Di10</p><p>签证   Hc16 (this coding is not included in the list of POS, and we keep it)</p><p>的       Bo29*      Ed01*      Kd01</p><p>馆       Dm05</p><p>领       Bk05*     Bq04*       Dn08*     Hf04      Hi27       Je14</p><p>中       Aj14*      Ca06*      Ca08*     Cb04     Cb05     Da05*    Ea03*    Ed06*</p><p>           Ed47*       Ie11*       Je13*     </p><p>     </p><p>    在       Hj19*       Ib03*       Ja05*      Jd01*     Jd02*     Ka12*    Kb01</p><p>    馆       Dm05</p><p>    领       Bk05*     Bq04*       Dn08*     Hf04      Hi27       Je14</p><p>    总       Ae10*     Ed56*       Ka11*     Ka15*     Ka29*(all of these codings are not </p><p>                included in the list of POS, and  we keep them all)</p><p>    纽约   ____</p><p>    驻       Hb04</p><p>    中国   Di02</p><p>    的       Bo29*      Ed01*      Kd01</p><p>    曼哈顿 ____</p><p>    位于   Jd02</p><p>    向       Cb01*     Ka10*     Kb01*(all of these codings are not </p><p>                included in the list of POS, and  we keep them all)</p><p>    涌       ____</p><p>    移民   ____</p><p>    新        Eb22*     Eb28*     Ka12</p><p>    人        Aa01       Ab02       Dd17      De01      Dn03</p><p>   上千    _____</p><p>   有        Eb02*       Ed61*     Jd01       Jd04        Jd07</p><p>   都        Cb25*       Di03*     Ka07      Ka12       Ka28</p><p>   天        Ca23*       Cb07*    Da24*    Dh01*      Ed51*    Ed57*    Bp26*      Dn05 </p><p>                Ka27</p><p>   几        Bp26*        Dn05      Ka27*</p><p>   连续     Ig03*         Ka11*(both of these two codings are not included in the list of POS, </p><p>                and we keep these two)</p><p>   以讹传讹    Ie01</p><p>   沸沸扬扬    Ef03</p><p>  得          Gc02*        Gc03*       Ie14*      Jc05*      Je12*     Ka15*     Kd01</p><p>  传           Dk26*       Hg01         Hi15       Ie01        Je03</p><p>  社区       ____</p><p>  华人       ____</p><p>  纽约       ____</p><p>  在           Hj19*       Ib03*       Ja05*      Jd01*     Jd02*     Ka12*    Kb01</p><p>  连日来   ____</p><p>  谣言      Da19</p><p>  的          Bo29*      Ed01*       Kd01</p><p>  资格      Dd16</p><p>  申请      Hc15</p><p>  护照      ____</p><p>  的         Bo29*      Ed01*       Kd01</p><p>  移民      ____</p><p>  新         Eb22*      Eb28*       Ka12</p><p>  大陆     Be01</p><p>  中国     Di02</p><p>  来自     Ja05</p><p> 放宽      ____</p><p> 领事馆  _____</p><p> 中国      Di02</p><p>We can see that before this dynamic POS-tagging-learned WSD, there are 118 words in the text with 225 large semantic categories represented by the 12 upper-case letters, 266 medium categories and 296 small categories. Before WSD, there are 37 mono-big-sense words, 34 mono-medium sense words and 34 mono-small-sense words . After  WSD, there are 166 large semantic categories, 191 medium categories and 207 small categories. And after WSD, there are 63 mono-large-sense words, 57 mono-medium-sense words and 62 mono-small-sense words. We can see from the following table that the average sense number is greatly decreased because of the WSD.</p><p>Table 4 Average Word Sense Before and After WSD</p><gap desc="table"/></div></div><div type="closer"><div type="bibliography"><head>Bibliography:</head><listBibl><bibl>1.Ji Donghong，Huang Changning．Sense Tagging of &lt;TongYiCiCiLin&gt; Using Itself．Laboratoryof Intelligent Technology and Systems, Tsinghua University</bibl><bibl>2.D.J. Arnold, Lorna Balkan, Siety Meijer, R.Lee Humphreys and Louisa Sadler Machine Translation: an Introductory Guide, Blackwells-NCC, London, 1994, ISBN: 1855542-17x. http://clwww.essex.ac.uk/MTbook/, chapter9</bibl><bibl>3.Martin Volk. 1997. Probing the lexicon in evaluating commercial MT systems. In Proc. of ACL/EACL Joint Conference, pages 112--119, Madrid. EAGLES Evaluation of Natural Language Processing Systems, http://www.ilc.pi.cnr.it/EAGLES96/browse.html#wg3</bibl><bibl>4.Nancy Ide, Jean Veronis, Introduction to the Special Issue on Word Sense Disambiguation: The State of Art.</bibl><bibl>5. 梅家驹，&lt;&lt;同义词词林&gt;&gt;，上海辞书出版社，1983              </bibl><bibl>6周强，余士汶，一个人机互助的汉语语料库多级加工处理系统，北大计算语言所学术研讨会论文</bibl><bibl> </bibl></listBibl></div><div type="footnotes"><note type="footnote" id="fn1">1 This trend can be clearly seen if we have a look at the word distribution in a dynamic corpus. For example, the PFR corpus of  "People's Daily"(Jan,1998) at &lt;http://www.icl.pku.edu.cn/Introduction/corpustagging.htm&gt;</note></div></div></body></text></TEI.2>