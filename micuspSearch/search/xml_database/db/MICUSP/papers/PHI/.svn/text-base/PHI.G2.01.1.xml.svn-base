<TEI.2><teiHeader><fileDesc><titleStmt><title>On Hempel's Account of Scientific Explanations</title><author>anonymized</author></titleStmt><wordcount>2541</wordcount><editionStmt><edition>Version 1.0</edition></editionStmt><extent>insert_extent</extent><publicationStmt><idno>PHI.G2.01.1</idno><publisher>The University of Michigan</publisher><pubPlace>Ann Arbor, Michigan, USA</pubPlace><date>2007</date><availability><p>The MICUSP project is owned by the Regents of the University of Michigan, who hold the copyright. The database has been developed by the English Language Institute. The original text submissions are stored electronically in the English Language Institute and may be consulted by bona fide researchers under special arrangements. The database is available by password on the World Wide Web for study, teaching and research purposes. Copies of the text files may be distributed, as long as either this statement of availability or the citation given below appears in the text. However, if any portion of this material is to be used for commercial purposes, such as for textbooks or tests, permission must be obtained in advance and a license fee may be required. For further information about copyright permissions, please inquire at micusp@umich.edu.</p><p>The recommended citation for MICASE is: Ädel, A., J. M. Swales, and J. Sielaff. (2007) The Michigan Corpus of Upper-level Student Papers. Ann Arbor, MI: The Regents of the University of Michigan.</p></availability></publicationStmt><seriesStmt><title>The Michigan Corpus of Upper-level Student Papers (MICUSP)</title></seriesStmt><sourceDesc><p>This document derives from an unpublished, student-authored paper contributed in electronic format.</p></sourceDesc></fileDesc><encodingDesc><projectDesc><p>For information on corpus compilation and encoding practices, see the MICUSP Handbook.</p></projectDesc><classDecl><taxonomy><category><catDesc>departments</catDesc><category id="BIO"><catDesc>Biology</catDesc></category><category id="CEE"><catDesc>Civil and Environmental Engineering</catDesc></category><category id="CLS"><catDesc>Classical Studies</catDesc></category><category id="ECO"><catDesc>Economics</catDesc></category><category id="EDU"><catDesc>Education</catDesc></category><category id="ENG"><catDesc>English</catDesc></category><category id="IOE"><catDesc>Industrial and Operational Engineering</catDesc></category><category id="LIN"><catDesc>Linguistics</catDesc></category><category id="MEC"><catDesc>Mechanical Engineering</catDesc></category><category id="NRE"><catDesc>Natural Resources</catDesc></category><category id="NUR"><catDesc>Nursing</catDesc></category><category id="PHI"><catDesc>Philosophy</catDesc></category><category id="PHY"><catDesc>Physics</catDesc></category><category id="POL"><catDesc>Political Science</catDesc></category><category id="PSY"><catDesc>Psychology</catDesc></category><category id="SOC"><catDesc>Sociology</catDesc></category></category><category><catDesc>student levels</catDesc><category id="G0"><catDesc>senior undergraduate</catDesc></category><category id="G1"><catDesc>first-year graduate</catDesc></category><category id="G2"><catDesc>second-year graduate</catDesc></category><category id="G3"><catDesc>third-year graduate and up</catDesc></category></category><category><catDesc>student-reported text types</catDesc><category id="CaseStudy"><catDesc>case study</catDesc></category><category id="Critique"><catDesc>critique</catDesc></category><category id="LiteratureReview"><catDesc>literature review</catDesc></category><category id="ResearchProposal"><catDesc>research proposal</catDesc></category><category id="ResponsePaper"><catDesc>response paper</catDesc></category><category id="TechnicalOrLabReport"><catDesc>technical report or lab report</catDesc></category><category id="TermPaper"><catDesc>term paper</catDesc></category><category id="OtherTextType"><catDesc>other</catDesc></category></category><category><catDesc>approximate preparation times</catDesc><category id="OneToThreeDays"><catDesc>1-3 days</catDesc></category><category id="FourToTenDays"><catDesc>4-10 days</catDesc></category><category id="TwoToThreeWeeks"><catDesc>2-3 weeks</catDesc></category><category id="ThreeToSixWeeks"><catDesc>3-6 weeks</catDesc></category><category id="OneFullSemester"><catDesc>one full semester</catDesc></category></category><category><catDesc>sources of feedback</catDesc><category id="NoFeedback"><catDesc>no feedback from others</catDesc></category><category id="PrimaryInstructor"><catDesc>primary course instructor</catDesc></category><category id="GSI"><catDesc>Graduate Student Instructor (teaching assistant)</catDesc></category><category id="WritingTutor"><catDesc>writing tutor</catDesc></category><category id="FriendOrClassmate"><catDesc>friend or classmate</catDesc></category><category id="OtherFeedback"><catDesc>other</catDesc></category></category></taxonomy></classDecl></encodingDesc><profileDesc><creation>Paper submitted to instructor in Apr 2007</creation><particDesc><person id="P239" sex="f" age="TwentyFourToThirty"><affiliation>Psychology</affiliation><firstLang>English</firstLang><dominantLang>English</dominantLang><englishInPrimarySchool value="YES"/><englishInSecondarySchool value="YES"/><englishInUndergraduate value="YES"/></person></particDesc><textClass><catRef target="PHI G2 Essay TermPaper OneToThreeDays NoFeedback"><classification><primary>Argumentative Essay</primary><secondary/></classification><classification><primary>Argumentative Essay</primary><secondary/></classification><features><feature type="Reference to sources"/></features></catRef></textClass></profileDesc><revisionDesc><change><date>2009-04-01</date><respStmt><name>Emily Lin</name><resp>coder</resp></respStmt><item>manual adjustment of tags</item></change><change><date>2007-05-22</date><respStmt><name>Gregory Garretson</name><resp>programmer</resp></respStmt><item>initial (automated) encoding in XML</item></change></revisionDesc></teiHeader><text><body><div type="opener"><head>On Hempel's Account of Scientific Explanations</head><p/></div><div type="main"><p>	What should a scientific explanation consist of?  This is a question that Carl Hempel tackles in his essay <q type="title">Laws and their role in scientific explanation.</q>  His prescription has played an important role in the history of the philosophy of science and arguably in science itself.  However, as I will discuss in this paper, his theory does not address some important concerns that can be raised with this criterion of explanatory relevance: the role of post hoc explanations and the role of human psychology.  As I shall address, Hempel's model is not explicit about these topics, but if it is the case that the model excludes post hoc explanations and explanations where human cognition does not allow the prediction of the explanandum based on the explanans, we would apparently be left with no explanation at all.</p><p>Hempel proposes two models of scientific explanations, both of which require the use of laws.  The first type Hempel discusses is the deductive-nomological explanation.  Under this model, a scientific explanation is a deductively valid argument, where the fact to be explained – called the explanandum – is the conclusion.  The premises consist both of laws of nature, and particular facts about the world.  These statements are called explanans.  An example of such an argument-cum-explanation would be:</p><list><item><i type="example">1 A gas exerts more pressure as its temperature increases, all else equal.</i></item><item><i type="example">2 The temperature today is higher than it was yesterday.				</i></item><item><i type="example">3 My tire pressure is higher today than it was yesterday, but I have not added air.</i></item></list><p>Statement 1 is a law of nature, or in Hempel's terminology a <q type="quote">covering law,</q> as it subsumes the explanandum 3.  Statement 2 is a particular fact about the world.  Statement 3 is the deductively valid conclusion of premises 1 and 2 (given some additional implied premises such as <q type="example">my tire contains a gas,</q><q type="example">my tire pressure gauge is accurate,</q> and the like).</p><p>The inductive-statistical explanation also models explanation as an argument, with laws of nature and particular facts as the premises and the explanandum as the conclusion.  However, in an I-S explanation, the argument does not have to be deductively valid; rather, it must be inductively strong – that is, that the premises must make the conclusion highly likely.  The I-S model allows for probabilistic laws to be used as premises.</p><p>	Hempel proposes two conditions that must be met for an explanation to be satisfactory.  The first of these criteria he calls <q type="quote">the requirement of explanatory relevance.</q>  By this, he means that the explanation must make one expect or believe that the fact to be explained did or would occur.  Hempel calls his second criterion <q type="quote">the requirement of testability,</q> meaning just that the statements used as premises in the explanation (the laws and particular facts) must be able to be empirically tested.</p><p>According to Hempel, laws are a necessary part of explanations because laws fill the requirement of explanatory relevance.  That is, laws provide the link whereby the explanandum is to be expected based upon the other explanans.  In a sense, scientific laws could be thought of as rigid, well-verified if-then statements.  <i>If</i> a gas is heated, <i>then</i> it will expand.  <i>If</i> a magnet is broken, <i>then</i> the two pieces will be magnets as well.  Hempel puts it this way <q type="quote">laws….are...statements of universal form [which] asserts a uniform connection between different empirical phenomena or between different aspects of an empirical phenomenon</q> (p. 309).</p><p>	Hempel's model of explanations seems quite simple and perhaps too straightforward to bring any major criticism against.  Upon reflection, however, it has some serious and problematic implications.  One question that can be raised against Hempel's view of explanation is, why are laws needed as premises/explanans rather than simply universal generalizations?  Indeed, what makes a law different from a universal generalization in the first place?  This is a fairly titanic question that Hempel hands off to other philosophers (at least in his essay “Laws and their Roles in Scientific Explanations).  I will do the same, except to note that Hempel fails to justify the inclusion of laws over universal generalizations.  He comments that laws are thought of as different from other universal generalizations because laws can be used to support counterfactuals.  However, he does not address why this is the case, nor why the property of supporting counterfactuals is necessary for explanation.</p><p>	Beyond this, we must ask, what is unique about Hempel's model in terms of description or prescription?  One feature seems the most prominent: his requirement that people must expect (be able to predict) the explanandum based upon the explanans – that is, the requirement of explanatory relevance.  This leads to two problems that I will address in turn: it does not seem to allow for post-hoc explanations, and it apparently does not allow for explanations where human cognitive limitations prevent the prediction of the explanandum from the explanans.</p><p>	The first issue is that the Hempel's criterion of explanatory relevance means that the model does not account for certain situations in which explanations are taken as acceptable (e.g., by the scientific community) but where the explanandum cannot be predicted from the explanans.  One example of this could be evolutionary explanations.  Suppose that one wants to explain how a species of moths came to have very long tongues.  A D-N style explanation would require premises (laws and particular facts) that would predict this.  But, though we can have extensive knowledge about the environment of these moths and the mechanisms of evolution, it does not seem possible to predict a particular adaptation.  It seems that in such cases, if we did not already know what sort of adaptation had occurred, the premises available to us would be something along the lines of:</p><list><item><i type="example">4 Having a trait that makes survival and reproduction more likely causes that trait to become more common in the species.  (Law)</i></item><item><i type="example">5 Moths of species X live in an environment where having a longer tongue would make survival and reproduction more likely. (Particular fact)</i></item><item><i type="example">6 Moths of species X live in an environment where having black wings would make survival and reproduction more likely. (Particular fact)</i></item><item><i type="example">7 Moths of species X live in an environment where having short antennae would make survival and reproduction more likely. (Particular fact)</i></item><item><i type="example">8 Etc., ad infinitum.</i></item></list><p>It is apparent that we could not predict the explanandum (that moths of species X have long tongues) based upon these premises.  However, we do find certain evolutionary explanations to be acceptable.  For the moths in question, the evolutionary explanation would go something like this:</p><list><item><i type="example">9 Moths of species X evolved in an environment where having a longer tongue would make survival and reproduction more likely. (Particular fact)</i></item><item><i type="example">10 Having a trait that makes survival and reproduction more likely causes that trait to become more common in the species.  (Law)</i></item><item><i type="example">11 Moths of species X randomly mutated such that some moths had longer tongues. (Particular fact)</i></item><item><i type="example">12 Moths of species X have very long tongues.</i></item></list><p>Here, the explanandum does follow from the explanans.  But in order to make this happen, we had to add a particular fact (11) post hoc.  The only way we know fact 11 is through an argument with the explanandum as a premise:</p><list><item><i type="example">13 Moths of species X have very long tongues</i></item><item><i type="example">14 Random mutation leading to a trait that makes survival and reproduction more likely causes that trait to become more common in the species.</i></item><item><i type="example">15 Moths of species X evolved in an environment where having a longer tongue would make survival and reproduction more likely.	</i></item><item><i type="example">16 Moths of species X must have randomly mutated such that some moths had longer tongues.</i></item></list><p>If Hempel's model of explanation bars this sort of post hoc reasoning (as it seems it should, since one should not be able to <i>assume</i> the conclusion in order to <i>predict</i> the conclusion), it would bar the argument we find acceptable as an explanation.  It is unclear how Hempel would respond to this problem.  He may conclude that the argument we take as an explanation isn't properly an explanation, and further, evolutionary explanations in general are nearly impossible to provide – the random nature of mutations prevents us from having knowledge of particular facts of mutations.  However, it seems that the post hoc explanation given is better than nothing, and does contribute to our understanding.</p><p>If Hempel's model does not accommodate our post hoc explanations, this would be a problem for his model.  The model requires explanation to involve prediction, but prediction and explanation are very different animals, with the key difference here being that facts learned post hoc can be (and regularly are) used for explanation.  Consider this scenario: you are in a casino with a friend.  Suddenly, near the slot machines, bells and whistles and lights go off, and a woman jumps up and down, whooping and yelling, <q type="example">hooray!</q>  Your friend asks, <q type="example">Why is she yelling like that?</q>  You take in the sights and deduce from the explanandum (the woman's yelling), and other particular facts (the bells and lights, your location in a casino, the woman's location near the slot machines, the nature of slot machines) that she must have won the jackpot.  You reply to your friend, <q type="example">I think she won the jackpot.</q>  Your friend is satisfied with your explanation, but you would not have been able to predict the yelling without knowing she won the jackpot, a fact you deduced using the yelling itself.</p><p>Many other scenarios can be given where the explanandum is required to deduce some fact used as part of the explanans, and where people take these as perfectly satisfactory explanations.  In order for Hempel to defend his model, he would need to show either how such accounts fit in his model, or why we should not accept such accounts as explanations when they seem to contribute to our understanding of how things happen.</p><p>The second issue I take with Hempel's model is that there seems to be a heavy psychological component in the requirement of explanatory relevance, but its importance is not addressed.  Suppose we have a situation where the explanandum is theoretically predictable from the explanans, but this prediction would require much more computing power or mental capacity than any person is capable of.  Is the account provided an explanation?  What if there was a situation where only people with particularly large working memory capacity could predict the explanandum from the explanans?  Is the account an explanation, but only for some people?  Do we want our conception of an <q type="soCalled">explanation</q> to depend upon the mental abilities of those we are explaining to?  The answer to this question is not readily apparent.  Further, and perhaps more pertinent to modern science, what if a person could only predict the explanandum from the explanans within his lifetime with the aid of a computer?  Does this count as an explanation?</p><p>One aspect of science that these questions seem particularly relevant to is connectionist models or neural networks in cognitive psychology.  Scientists have attempted to understand, explain, model, or simulate (depending upon which scientist you ask) how the human brain processes information by using networks of idealized neurons instantiated on a computer.  The networks consist of many <q type="soCalled">units</q> that mimic the actions of human neurons.  Each unit is connected to many others and a unit can send signals to other units it is connected to.  An individual unit performs only the following functions: it adds up the strengths of signals it is receiving from other neurons, weighting them based upon the strength of its connection with the transmitting unit.  Then, the unit calculates (based upon an assigned function) what strength signal to send out based upon the inputs it received, and then sends out this signal.  The strength of a connection can be changed through intervention (i.e., the experimenter changes the weight) or though a learning mechanism (e.g., back propagation) wherein the network changes its own connection weights based upon programmed rules.  Some units in a network get input from the environment, and some output their activity into the environment.</p><p>A network built of these simple units, adhering to these simple rules can be made (through appropriate learning mechanisms and input) to do remarkable things and become quite complicated.  For example, one network (discussed by Hinton, 1992) can recognize handwritten digits.  A typical network may consist of thousands of units with hundreds of thousands of connections.  But, if a person was made fully aware of all the parameters of the network (how the units work, all the connections between units and their weights, the rules by which the weights change, the input-output conversion function the units implement, and the input the network has received) she could not predict what the output would be – at least not without using a computer to implement the network – though theoretically, the explanandum is predictable from the explanans.  So, should we consider the parameters of the network an explanation?  Even more, should we by analogy take – as some scientists do – the features of a human's neurons to be explanations of how the brain works?</p><p>Some scientists (most notably McCloskey, 1991) have argued that appealing to the parameters of neural networks is no explanation, since one still does not really understand the inner workings of the network.  He makes an analogy to a black box, saying that if you do not understand what happens between input and output, one cannot refer to the black box as an explanation.  McCloskey's objection is that he does not <i>understand</i> how these networks work.  He discusses a particular network (Seidenberg &amp; McClelland's 1989 model of word recognition and naming) designed for word recognition.  He argues that one cannot answer questions such as <q type="example">why doesn't the network distinguish 'shin' from 'chin'?</q> by appealing to the parameters of the network, and it is not possible to give an explanation on appropriately understandable theoretical levels.  For example, one cannot ask of a network, <q type="example">does it perform orthography-phonology conversion in a single mechanism?</q> and get an understandable answer.</p><p>	If we cannot appeal to the parameters of the neural network to explain why it does not distinguish 'shin' from 'chin' – since a person could not predict this outcome based upon the parameters – what else can we use to explain this?  The apparent answer is nothing.  Failing an explanation that fulfils Hempel's explanatory relevance criterion, are we left with no explanation at all?  This seems unsatisfactory.  Again we have a circumstance where Hempel's model of explanation may bar an account as explanation but fail to provide an alternative.  Surely appealing to a neural network's parameters to explain its behavior is better than no explanation at all.  It is unclear how Hempel would accommodate these charges. </p><p>	To sum, Hempel's D-N and I-S models of explanation fail to explicitly address the two concerns raised: the role of post hoc explanations and the role of human cognition in explanation.  The model's apparent stance, though, would give us no explanation at all in situations where explanations not in accord with the model are typically given.  This is a problem for the model and warrants further discussion.</p></div><div type="closer"><div type="bibliography"><head>References</head><listBibl><bibl>Hempel, C.G.  (1966).  Philosophy of Natural Science.  Englewood Cliffs: Prentice-Hall.</bibl><bibl>Hinton, G. E. (1992). How neural networks learn from experience.  Scientific American, 267(3), 105-109.</bibl><bibl>McCloskey, M. (1991). Networks and theories - The place of connectionism in cognitive science. Psychological Science, 2(6):387-395. </bibl></listBibl></div></div></body></text></TEI.2>